{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Preparation: Virtual environment and packages**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1 install web browser driver**   \n",
    "- install one of the following according to your web browser:   \n",
    "    selenium driver (firefox): https://github.com/mozilla/geckodriver/releases   \n",
    "    selenium driver (Chrome): https://sites.google.com/a/chromium.org/chromedriver/downloads\n",
    "- extract file\n",
    "- make it executable (Linux)\n",
    "\t`chmod +x geckodriver`\n",
    "- move it to appropriate location\n",
    "\t`sudo mv geckodriver (appropriate location)`   \n",
    "\t/usr/bin   \n",
    "\t/usr/local/bin\n",
    "- remove file and empty folders (in download folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2 create virtual environment**\n",
    "- install/update 'pip': `python3 -m pip install --user -U pip`\n",
    "- install 'virtualenv' package: `python3 -m pip install --user -U virtualenv`\n",
    "- create folder\n",
    "- open folder in terminal\n",
    "- create virtual environment: `virtualenv (environment_name)`\n",
    "- activate virtual environment: `source (environment_name)/bin/activate (linux)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3 install needed packages**\n",
    "- scrappers: `python3 -m pip install -U bs4 selenium requests lxml lightrdf`\n",
    "- analysis: `python3 -m pip install -U jupyter matplotlib numpy pandas scipy scikit-learn pystan`\n",
    "- nlp: `python3 -m pip install -U spacy nltk itertools`\n",
    "- file control: `python3 -m pip install -U os shutil send2trash re glob`\n",
    "- already installed: `json`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.4 save requirements.txt:** `pip freeze > requirements.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Web-scrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1 Load modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver as wd\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.common import exceptions\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2 Design scrapper**\n",
    "- Define function `handle_stale` to handle `StaleElementReferenceException`\n",
    "- Define function `handle_noclick` to handle `ElementNotInteractableException`\n",
    "- The main scrapper is built as function **`bulk_download`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "# handle StaleElementReferenceException\n",
    "############################################\n",
    "def handle_stale(b, e, cn, i):\n",
    "    try:\n",
    "        Select(e).select_by_index( i )\n",
    "    except exceptions.StaleElementReferenceException:\n",
    "        e = b.find_element_by_class_name( cn )\n",
    "        Select( e ).select_by_index( i )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "# handle ElementNotInteractableException\n",
    "############################################\n",
    "def handle_noclick(b, xp, id_):\n",
    "    try:\n",
    "        b.find_element_by_xpath( xp ).click()\n",
    "        b.find_element_by_id( id_ ).click()\n",
    "    except exceptions.ElementNotInteractableException:\n",
    "        pass\n",
    "    except exceptions.NoSuchElementException:\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "# bulk download\n",
    "############################################\n",
    "def bulk_download(webpage, save_dir, driver='Firefox',\n",
    "                  file_type=\"application/csv,text/csv,text/comma-separated-values\"):\n",
    "\n",
    "##  # defines profile and browser\n",
    "    if driver=='Firefox':\n",
    "        \n",
    "##      # profile for autosaving (Firefox)\n",
    "        profile = wd.FirefoxProfile()\n",
    "        profile.set_preference(\"browser.download.folderList\", 2)\n",
    "        profile.set_preference(\"browser.download.manager.showWhenStarting\", False)\n",
    "        profile.set_preference(\"browser.download.dir\", save_dir)\n",
    "        profile.set_preference(\"browser.helperApps.neverAsk.saveToDisk\", file_type)\n",
    "\n",
    "##      # start webdriver\n",
    "        browser = wd.Firefox(profile)\n",
    "\n",
    "    elif driver=='Chrome':\n",
    "##      # WORK IN PROGRESS\n",
    "##      # profile for autosaving (Chrome)\n",
    "        profile = wd.ChromeOptions()\n",
    "        prefs = {'download.prompt_for_download': False,\n",
    "                 'safebrowsing.enabled': False,\n",
    "                 'safebrowsing.disable_download_protection': True,\n",
    "                 \"profile.default_content_settings.popups\": 0,\n",
    "                 \"download.default_directory\": save_dir,\n",
    "                 'download.directory_upgrade': True,}\n",
    "        profile.add_experimental_option('prefs', prefs)\n",
    "\n",
    "##      # start webdriver \n",
    "        browser = wd.Chrome(profile)\n",
    "        \n",
    "    else:\n",
    "        raise SystemExit\n",
    "\n",
    "\n",
    "##  # get webpage\n",
    "    browser.get(webpage)\n",
    "    time.sleep(1)\n",
    "\n",
    "##  # accept cookies\n",
    "    browser.find_element_by_class_name(\"check\").click()\n",
    "\n",
    "##  # open dropdown historical\n",
    "    browser.execute_script(\"window.scrollTo(0,7500)\")\n",
    "    browser.find_element_by_xpath(\"/html/body/div[2]/main/div[8]/div[1]\").click()\n",
    "    \n",
    "##  # select years, months and days\n",
    "##  # element: year\n",
    "    yEl = browser.find_element_by_class_name(\"ui-datepicker-year\")\n",
    "    yOp = yEl.find_elements_by_tag_name(\"option\")\n",
    "\n",
    "    for y in range( len(yOp) ):\n",
    "        year = 2017 + y\n",
    "        handle_stale(b=browser, e=yEl, cn=\"ui-datepicker-year\", i=y)\n",
    "       \n",
    "##      # element: month\n",
    "        mEl = browser.find_element_by_class_name(\"ui-datepicker-month\")\n",
    "        mOp = mEl.find_elements_by_tag_name(\"option\")\n",
    "\n",
    "        for m in range( len(mOp) ):\n",
    "            if y==0: # year 2006 starts on june = 0\n",
    "                month = m + 6\n",
    "            else:\n",
    "                month = m + 1\n",
    "            \n",
    "            handle_stale(b=browser, e=mEl, cn=\"ui-datepicker-month\", i=m)\n",
    "\n",
    "##          # element: day of the week\n",
    "            main_xpath = \"/html/body/div[2]/main/div[8]/div[2]/div/div/div/table/tbody/\"\n",
    "\n",
    "            for w in range(6):\n",
    "                w_day = \"tr[\" + str(w + 1) + \"]/td[5]\" # '5' means active\n",
    "                w_xpath = main_xpath + w_day\n",
    "                calButton = \"download_calendarButton\"\n",
    "                handle_noclick(b=browser, xp=w_xpath, id_=calButton)\n",
    "       \n",
    "##              # check\n",
    "                print(\"donwload corresponds to: %s/%s, week: %s\" % (year, month, w) )\n",
    "                \n",
    "    browser.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3 RUN**   \n",
    "- We are happy to run the function to download all raw files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change working path\n",
    "sys.path.append('/home/jriveraespejo/Desktop/project_europa/notebooks')\n",
    "file_data = '/home/jriveraespejo/Desktop/project_europa/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bulk download\n",
    "# download all ECB files into raw folder\n",
    "web1 = \"https://www.ecb.europa.eu/mopo/implement/app/html/index.en.html#cspp\"\n",
    "save1 = file_data + 'raw'\n",
    "bulk_download(webpage=web1, save_dir=save1, file_type=filet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Web-scrapping (Alternative solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Short Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a rather fast solution compared with `selenium`. By extracting features from the link addresses on which ECB saves their CSPP holdings csv files, together with `requests` module visiting url, all files are automatically saved into the target directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import timedelta, date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1 Acquire all Friday dates of filing**\n",
    "- Acquire all Friday dates from `2017-06-23` until today. If a certain date is Friday (when ECB published purchasing information), then we add it to a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def daterange(start_date, end_date):\n",
    "    for n in range(int((end_date - start_date).days)):\n",
    "        yield start_date + timedelta(n)\n",
    "\n",
    "start_date = date(2017, 6, 23)\n",
    "end_date = date(2020, 3, 29)\n",
    "\n",
    "date_list = []\n",
    "\n",
    "for everyday in daterange(start_date, end_date):\n",
    "    if everyday.weekday() == 4:\n",
    "        date_list.append(everyday.strftime(\"%Y%m%d\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2 Prepare URL list**\n",
    "- We first define a list `CSV_URL` to store all website links that can directly access to the specific csv documents. Then we would like to declare the `directory` in which we save those csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## make links into a list object CSV_URL\n",
    "url_source = 'https://www.ecb.europa.eu/mopo/pdf/'\n",
    "url_category = 'CSPPholdings_'\n",
    "url_file_format = '.csv'\n",
    "\n",
    "CSV_URL = []  # date list to iterate through\n",
    "for i in range(len(date_list)):\n",
    "    CSV_URL.append(url_source + url_category + date_list[i] + url_file_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3 First time scrapping**\n",
    "- Web scraping part using a loop and `request` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save option --> change it to your directory!!\n",
    "directory = r'/Users/jingpuchen/Desktop/KU Leuven/Semester4/Modern Data Analytics/project/web_scraping/csv/'\n",
    "\n",
    "for i in range(len(CSV_URL)):\n",
    "    resp = requests.get(CSV_URL[i])  # get access to csv file\n",
    "    df = pd.read_csv(io.StringIO(resp.text))  # convert to text format\n",
    "#    df.dropna(axis=0,inplace=True)  # drop NA values ???????\n",
    "    df.to_csv(directory + date_list[i] + '.csv',\n",
    "               index = False,\n",
    "               header= True)  # without variable names --> add it later on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.4 Second time scrapping**\n",
    "- \n",
    "Why start scraping again? Because since 2020-03-30, ECB applied a new naming format, causing the parser fail to read in csv files if following the old naming pattern. Therefore, even though it is no a genius way to scrap the items again, it works for me. Also notice that there is not purchasement on two fridays: `20201225,20210101`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date_1 = date(2020, 3, 30)\n",
    "end_date_1 = date(2021, 4, 10)\n",
    "\n",
    "date_list_1 = []\n",
    "\n",
    "for everyday in daterange(start_date_1, end_date_1):\n",
    "    if everyday.weekday() == 4:\n",
    "        date_list_1.append(everyday.strftime(\"%Y%m%d\"))\n",
    "\n",
    "date_list_1 = [d for d in date_list_1 if d not in ('20201225','20210101')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## make links into a list object CSV_URL\n",
    "url_source_1 = 'https://www.ecb.europa.eu/mopo/pdf/'\n",
    "url_category_1 = 'CSPP_PEPP_corporate_bond_holdings_'\n",
    "url_file_format_1 = '.csv'\n",
    "\n",
    "CSV_URL_1 = []  # date list to iterate through\n",
    "for i in range(len(date_list_1)):\n",
    "    CSV_URL_1.append(url_source_1 + url_category_1 + date_list_1[i] + url_file_format_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save option\n",
    "directory = r'/Users/jingpuchen/Desktop/KU Leuven/Semester4/Modern Data Analytics/project/web_scraping/csv/'\n",
    "\n",
    "for i in range(len(CSV_URL_1)):\n",
    "    resp_1 = requests.get(CSV_URL_1[i])  # get access to csv file\n",
    "    df_1 = pd.read_csv(io.StringIO(resp_1.text))  # convert to text format\n",
    "    df_1.to_csv(directory + date_list_1[i] + '.csv',\n",
    "               index = False,\n",
    "               header= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we are done with downloading all csv files until the last release from `2020-04-09`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Append 198 `.csv` items including dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1 Load modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, re, chardet\n",
    "import pandas as pd\n",
    "from statistics import mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2 Design merge function**\n",
    "- Define function `clean_weird` to standardise text encoding system\n",
    "- Define function `merge_csv` to perform concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "# clean_weird function\n",
    "############################################\n",
    "def clean_weird(vector_dirty, extra=False):\n",
    "\n",
    "##  # ============================        \n",
    "##  # ORDER MATTERS HERE\n",
    "##  # ============================ \n",
    "##  # strip and to lower\n",
    "    vector_clean = vector_dirty.str.strip()\n",
    "    vector_clean = vector_clean.str.lower()\n",
    "\n",
    "##  # city names at the end\n",
    "    vector_clean = vector_clean.str.replace(r'(,\\s\\w+(\\s\\w+)?)$', '', regex=True)\n",
    "\n",
    "##  # ============================\n",
    "##  # remove weird symbols\n",
    "##  # ============================\n",
    "    vector_clean = vector_clean.str.replace(r'á','a', regex=True)\n",
    "    vector_clean = vector_clean.str.replace(r'ã','a', regex=True)\n",
    "    vector_clean = vector_clean.str.replace(r'ä','a', regex=True)\n",
    "    vector_clean = vector_clean.str.replace(r'é','e', regex=True)\n",
    "    vector_clean = vector_clean.str.replace(r'ë','e', regex=True)\n",
    "    vector_clean = vector_clean.str.replace(r'É','E', regex=True)\n",
    "    vector_clean = vector_clean.str.replace(r'í','i', regex=True)\n",
    "    vector_clean = vector_clean.str.replace(r'ó','o', regex=True)\n",
    "    vector_clean = vector_clean.str.replace(r'ö','o', regex=True)\n",
    "    vector_clean = vector_clean.str.replace(r'ü','u', regex=True)\n",
    "    vector_clean = vector_clean.str.replace(r'ñ','n', regex=True)\n",
    "    \n",
    "##  # ============================\n",
    "##  # remove company designations\n",
    "##  # ============================\n",
    "##  # see:\n",
    "##  # https://www.corporateinformation.com/Company-Extensions-Security-Identifiers.aspx\n",
    "##  # https://www.nathantrust.com/insights/comprehensive-guide-to-a-designated-activity-company-dac\n",
    "    if extra==True:\n",
    "\n",
    "##      # combos: as,sl,scs,sa,sac,sau,sas,spa,sanv, etc. (with and without intermediate . or /)\n",
    "        s_chars = r'(a\\W?s\\W?|s\\W?((a|e|p|c|l)\\W?)?((a|s|u)\\W?)?\\W?(n\\W?v\\W?)?(r\\W?l\\W?)?)$'\n",
    "        vector_clean = vector_clean.str.replace(s_chars, '', regex=True)\n",
    "\n",
    "##      # combos: nv,nvsa,bv,oyj,ltd, etc. (with and without intermediate . or /)\n",
    "        s_chars = r'((n|b)\\W?v\\W{0,2}?(s\\W?a\\W?)?|o\\W?y\\W?j\\W?|l\\W?t\\W?d\\W?)$'\n",
    "        vector_clean = vector_clean.str.replace(s_chars, '', regex=True)\n",
    "\n",
    "##      # combos: cvba,ag,plc,dac, etc. (with and without intermediate . or /)\n",
    "        s_chars = r'(c\\W?v\\W?b\\W?a\\W?|a\\W?g\\W?|p\\W?l\\W?c\\W?|d\\W?a\\W?c\\W?)$'\n",
    "        vector_clean = vector_clean.str.replace(s_chars, '', regex=True)\n",
    "\n",
    "##      # combos: ,(g)mbh, kgaa, etc. (with and without intermediate . or /)\n",
    "        s_chars = r'((g\\W?)?m\\W?b\\W?h\\W?|k\\W?g\\W?a\\W?a\\W?)$'\n",
    "        vector_clean = vector_clean.str.replace(s_chars, '', regex=True)\n",
    "\n",
    "##      # specifics\n",
    "        s_chars = r'(\\W(sa)\\s(\\wt)\\W(expl)\\W(p)\\W(g)\\W(cl)\\W)$'\n",
    "        vector_clean = vector_clean.str.replace(s_chars, '', regex=True)\n",
    "        \n",
    "        s_chars = r'(\\W(soc)\\W(an)\\W(d)\\W(gest)\\W(st)\\W(d)\\W(sec)\\W)$'\n",
    "        vector_clean = vector_clean.str.replace(s_chars, '', regex=True)\n",
    "\n",
    "    vector_clean = vector_clean.str.replace(r'-',' ', regex=True)\n",
    "    vector_clean = vector_clean.str.replace(r'\\s{2,}',' ', regex=True)\n",
    "    vector_clean = vector_clean.str.replace(r'[^\\w\\s]','', regex=True)\n",
    "    vector_clean = vector_clean.str.strip()\n",
    "    \n",
    "    return(vector_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "# function merge_csv\n",
    "############################################\n",
    "def merge_csv(save_dir, file_dir, file_name):\n",
    "##      # location\n",
    "        os.chdir(file_dir)\n",
    "                \n",
    "##      # list files\n",
    "        all_files = [i for i in glob.glob(\"*.csv\")]\n",
    "\n",
    "##      # regular expression for date\n",
    "        regex = re.compile(r'\\d+')\n",
    "\n",
    "##      # iterating through data\n",
    "        all_df = [] # to concatenate all data\n",
    "        encode = [] # to save all encodings\n",
    "        \n",
    "        for file in all_files:\n",
    "##              # check encoding of files: open first 10'000 bytes                 \n",
    "                with open(file, 'rb') as rawdata:\n",
    "                        encoding = chardet.detect(rawdata.read(10000))\n",
    "##                print(encoding)\n",
    "##                # 73% of confidence in each file\n",
    "                        \n",
    "                encode.append(encoding['encoding']) # to use in final file\n",
    "\n",
    "##              # load data frame\n",
    "                df = pd.read_csv(file, sep=',', encoding=encoding['encoding'])\n",
    "\n",
    "##              # eliminating unnecessary columns\n",
    "##              # some files have extra empty colums\n",
    "                if df.shape[1] > 5:\n",
    "                        df.drop(df.iloc[:, 5:], axis=1, inplace=True)\n",
    "\n",
    "##              # equalizing column names\n",
    "                df.columns = ['NCB','ISIN_CODE','ISSUER_NAME','MATURITY_DATE','COUPON_RATE']\n",
    "                \n",
    "##              # eliminating noninformative rows\n",
    "                idxNum = df[ df.ISSUER_NAME.isnull() ].index\n",
    "                df = df.drop(index=idxNum)\n",
    "\n",
    "                idxNum = df.ISSUER_NAME.str.contains('(d|D)ummy')\n",
    "                idxNum = idxNum.fillna(False)\n",
    "                idxNum = df[ idxNum ].index\n",
    "                df = df.drop(index=idxNum)\n",
    "                \n",
    "##              # adding file date\n",
    "                df['file_date'] = regex.findall(file) * df.shape[0]\n",
    "\n",
    "##              # merging\n",
    "                all_df.append(df)\n",
    "                merged_df = pd.concat(all_df, ignore_index=True, sort=True)\n",
    "\n",
    "##      # sorting by date\n",
    "        merged_df = merged_df.sort_values(by='file_date')\n",
    "\n",
    "##      # creting column with new names\n",
    "        merged_df[\"Name1\"] = clean_weird( merged_df['ISSUER_NAME'], extra=False)\n",
    "        merged_df[\"Name2\"] = clean_weird( merged_df['ISSUER_NAME'], extra=True)\n",
    "        \n",
    "##      # saving data\n",
    "##      # use most repeated encoding\n",
    "        final_encode = mode(encode)\n",
    "        full_path = '1_' + save_dir + file_name + '.csv'\n",
    "        merged_df.to_csv(full_path, index=False, encoding=final_encode)\n",
    "        \n",
    "        print('finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.3 RUN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge csv's\n",
    "# produced files: '1_CSPPholdings_201706_2021.csv'\n",
    "file2 = file_data + 'raw'\n",
    "file3 = file_data + 'processed/'\n",
    "name1 = \"CSPPholdings_201706_2021\"\n",
    "merge_csv(file_dir=file2, save_dir=file3, file_name=name1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Pull from PermID API and join data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.1 Create an account in https://permid.org/**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.2 Match entity's names and instruments (manual process)**\n",
    "- Web: https://permid.org/match\n",
    "- There is one file for companies' names (name file):\n",
    "    - with legal designation (with_\\*)\n",
    "    = without legal designation (without_\\*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.2.1 Load modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import pandas as pd\n",
    "import json, re, chardet, string, time, requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.2.2 Define function `match_format`**\n",
    "- to produce the correct csv, per (name file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "# match_format function\n",
    "############################################\n",
    "def match_format(file_dir, file_name, save_dir, save_name):\n",
    "    \n",
    "##  # full path for file\n",
    "    full_path = file_dir + file_name + '.csv'\n",
    "    \n",
    "##  # check encoding of files: open first 10'000 bytes                 \n",
    "    with open(full_path, 'rb') as rawdata:\n",
    "        encoding = chardet.detect(rawdata.read(10000))\n",
    "##    print(encoding)\n",
    "##    # 73% of confidence\n",
    "        \n",
    "##  # load data\n",
    "    df = pd.read_csv(full_path, sep=',', encoding=encoding['encoding'])\n",
    "\n",
    "##  # put info in the right format    \n",
    "##  # see https://permid.org/match:\n",
    "##  # 'Organization' and 'Download Template' buttons\n",
    "    df = df.drop(['COUPON_RATE','ISIN_CODE','ISSUER_NAME','MATURITY_DATE','NCB','file_date'], axis = 1)\n",
    "##  # drop NCB (country) as it has missmatchs\n",
    "    \n",
    "##  # add empty info\n",
    "    df['LocalID'] = ''\n",
    "    df['Standard Identifier'] = ''\n",
    "    df['Country'] = ''\n",
    "    df['Street'] = ''\n",
    "    df['City'] = ''\n",
    "    df['PostalCode'] = ''\n",
    "    df['State'] = ''\n",
    "    df['Website'] = ''\n",
    "\n",
    "##  # remove duplicates and sort\n",
    "    df = df.drop_duplicates(subset=['Name1','Name2'])\n",
    "    df = df.sort_values(by=['Name1'])\n",
    "    \n",
    "##  # re-order columns\n",
    "##  # names with and without company designations\n",
    "    df_main = df[['Name1','Name2']]\n",
    "    \n",
    "##  # names with company designations\n",
    "    df_with = df[['LocalID','Standard Identifier','Name1','Country','Street','City','PostalCode','State','Website']]\n",
    "    df_with.rename(columns={\"Name1\":\"Name\"}, inplace=True)\n",
    "\n",
    "##  # names without company designations    \n",
    "    df_without = df[['LocalID','Standard Identifier','Name2','Country','Street','City','PostalCode','State','Website']]\n",
    "    df_without.rename(columns={\"Name2\":\"Name\"}, inplace=True)\n",
    "    \n",
    "##  # saving data\n",
    "    full_path = '2_' + save_dir + 'main_' + save_name + '.csv'\n",
    "    df_main.to_csv(full_path, index=False, encoding=encoding['encoding'])\n",
    "\n",
    "    full_path = '2_' + save_dir + 'with_' + save_name + '.csv'\n",
    "    df_with.to_csv(full_path, index=False, encoding=encoding['encoding'])\n",
    "\n",
    "    full_path = '2_' + save_dir + 'without_' + save_name + '.csv'\n",
    "    df_without.to_csv(full_path, index=False, encoding=encoding['encoding'])\n",
    "\n",
    "    print('finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.3 Join all matched information**\n",
    "- Define function `match_info` to join all (name files) produced in the previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "# match_info function\n",
    "############################################\n",
    "def match_info(file_dir, file_name, save_dir, save_name):\n",
    "\n",
    "##  # paths for all files\n",
    "    full_path1 = '2_' + file_dir + 'main_' + file_name + '.csv'\n",
    "    full_path2 = '3_' + save_dir + 'with_match_' + file_name + '.csv'\n",
    "    full_path3 = '3_' + save_dir + 'without_match_' + file_name + '.csv'\n",
    "\n",
    "##  # check encoding of files: open first 10'000 bytes                 \n",
    "    with open(full_path2, 'rb') as rawdata: # the most common file\n",
    "        encoding = chardet.detect(rawdata.read(10000))\n",
    "##    print(encoding)\n",
    "##    # 73% of confidence\n",
    "        \n",
    "##  # load data\n",
    "    df_main = pd.read_csv(full_path1, sep=',', encoding=encoding['encoding'])\n",
    "\n",
    "    df_with = pd.read_csv(full_path2, sep=',', encoding=encoding['encoding'])\n",
    "    df_with.rename(columns={\"Match OpenPermID\":'OpenPermID_1',\n",
    "                            'Match OrgName':'OrgName_1',\n",
    "                            'Match Score':'Score_1',\n",
    "                            'Match Level':'Level_1'}, inplace=True)\n",
    "\n",
    "    df_without = pd.read_csv(full_path3, sep=',', encoding=encoding['encoding'])\n",
    "    df_without.rename(columns={\"Match OpenPermID\":'OpenPermID_2',\n",
    "                               'Match OrgName':'OrgName_2',\n",
    "                               'Match Score':'Score_2',\n",
    "                               'Match Level':'Level_2'}, inplace=True)\n",
    "\n",
    "##  # add columns of interest\n",
    "    df_final = pd.DataFrame([])\n",
    "    df_final['Name_1'] = df_main['Name1']\n",
    "    df_final = df_final.join( df_with.iloc[:,1:5] )\n",
    "    df_final['Name_2'] = df_main['Name2']\n",
    "    df_final = df_final.join( df_without.iloc[:,1:5] )\n",
    "    df_final['EqualOrgName'] = ( df_final['OrgName_1'] == df_final['OrgName_2'] )\n",
    "\n",
    "##  # saving data\n",
    "    full_path = '4_' + save_dir + 'main_match_' + save_name + '.csv'\n",
    "    df_final.to_csv(full_path, index=False, encoding=encoding['encoding'])\n",
    "\n",
    "    print('finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.4 Pull information of entities**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.4.1 Load modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json, re, chardet, string, time, requests, lightrdf\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from selenium import webdriver as wd\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.common import exceptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.4.2 Define several handlers to let things done smoothly**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "# handlers\n",
    "############################################\n",
    "\n",
    "## if the xpath is not clickable\n",
    "def handle_noclick_xp(b, xp_):\n",
    "    try:\n",
    "        b.find_element_by_xpath( xp_ ).click()\n",
    "    except exceptions.ElementNotInteractableException:\n",
    "        pass\n",
    "    except exceptions.NoSuchElementException:\n",
    "        pass\n",
    "\n",
    "## if the id is not clickable\n",
    "def handle_noclick_id(b, id_):\n",
    "    try:\n",
    "        b.find_element_by_id( id_ ).click()\n",
    "    except exceptions.ElementNotInteractableException:\n",
    "        pass\n",
    "    except exceptions.NoSuchElementException:\n",
    "        pass\n",
    "\n",
    "## test the existence of the multiple 'divs'\n",
    "def handle_noclick_extra(b, xp_start, xp_end, divs):\n",
    "    for div in divs:\n",
    "        try:\n",
    "            xp = xp_start + str(div) + xp_end\n",
    "            b.find_element_by_xpath( xp ).click()\n",
    "        except exceptions.ElementNotInteractableException:\n",
    "            next\n",
    "        except exceptions.NoSuchElementException:\n",
    "            next\n",
    "        else:\n",
    "            break\n",
    "\n",
    "## handle error by xpath\n",
    "def handle_error_css(b, css_):\n",
    "    try:\n",
    "        text = b.find_element_by_css_selector(css_).text\n",
    "    except exceptions.ElementNotInteractableException:\n",
    "        text = 'No error'\n",
    "    except exceptions.NoSuchElementException:\n",
    "        text = 'No error'\n",
    "    return(text)\n",
    "\n",
    "\n",
    "## reload page with 'Unexpected error occurred'\n",
    "## NEEDS REVIEWING\n",
    "def handle_web(b, w):\n",
    "##    xpath = \"/html/body/div[3]/section/div/div/div/div/div[1]/div/div/h1\"\n",
    "    csspath = \"div.About-title h1.heading-2.ng-binding\"\n",
    "    error_text = handle_error_css(b=b, css_=csspath)\n",
    "    print(error_text)\n",
    "\n",
    "    j = 1\n",
    "    while error_text == 'Unexpected error occurred':\n",
    "        j += 1\n",
    "        b.get(w)\n",
    "        time.sleep(10)\n",
    "        error_text = handle_error_css(b=b, css_=csspath)\n",
    "        print(error_text + \" at \" + str(j) + \" try\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.4.3 Define function: `match_data`** to scrap through PermID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "# match_data function\n",
    "############################################\n",
    "def match_data(wm, un, pw, file_dir, file_name, save_dir, save_name,\n",
    "               rows, round_=1, driver='Firefox',\n",
    "               ft=\"application/csv,text/csv,text/comma-separated-values\"):\n",
    "\n",
    "##  # ==========================\n",
    "##  # open file of pages\n",
    "##  # ==========================\n",
    "\n",
    "##  # first time of retrieving\n",
    "    if round_==1:\n",
    "        \n",
    "##      # path for file\n",
    "        full_path = file_dir + '4_main_match_' + file_name + '.csv'\n",
    "    \n",
    "##      # check encoding of files: open first 10'000 bytes                 \n",
    "        with open(full_path, 'rb') as rawdata:\n",
    "            encoding = chardet.detect(rawdata.read(10000))\n",
    "##      print(encoding)\n",
    "##      # 73% of confidence\n",
    "        \n",
    "##      # load data\n",
    "        df = pd.read_csv(full_path, sep=',', encoding=encoding['encoding'])\n",
    "\n",
    "##      # creating storage\n",
    "        df[\"Primary Industry\"] = '0'\n",
    "        df[\"Primary Bussiness\"] = '0'\n",
    "        df[\"Primary Economic\"] = '0'\n",
    "        df[\"Domiciled\"] = '0'\n",
    "        df[\"Incorporated\"] = '0'\n",
    "        df[\"TRBC code\"] = '0'\n",
    "        df[\"Primary Industry description\"] = '0'\n",
    "        obs = rows\n",
    "\n",
    "##      # to check 'nan' first\n",
    "        nan_org1 = df.OpenPermID_1.isna()\n",
    "        nan_org2 = df.OpenPermID_2.isna()\n",
    "        nan_org = nan_org1 & nan_org2\n",
    "\n",
    "##      # save path\n",
    "        full_path = save_dir + '5_info_match_round1_' + save_name + '.csv'\n",
    "        \n",
    "##  # second time of retrieving\n",
    "    elif round_==2:\n",
    "\n",
    "##      # path for file\n",
    "        full_path = file_dir + '5_info_match_round1_' + file_name + '.csv'\n",
    "    \n",
    "##      # check encoding of files: open first 10'000 bytes                 \n",
    "        with open(full_path, 'rb') as rawdata:\n",
    "            encoding = chardet.detect(rawdata.read(10000))\n",
    "##      print(encoding)\n",
    "##      # 73% of confidence\n",
    "        \n",
    "##      # load data\n",
    "        df = pd.read_csv(full_path, sep=',', encoding=encoding['encoding'])\n",
    "        print()\n",
    "        \n",
    "##      # check for empty values\n",
    "        nan_org1 = df.OpenPermID_1.isna()\n",
    "        nan_org2 = df.OpenPermID_2.isna()\n",
    "        nan_org = nan_org1 & nan_org2\n",
    "        obs = df[ (df[\"Primary Industry\"] == '0') & (nan_org!=True) ].index.tolist()\n",
    "        print('checking for ' + str( len(obs) ) + ' observations')\n",
    "\n",
    "##      # save path\n",
    "        full_path = save_dir + '5_info_match_round2_' + save_name + '.csv'\n",
    "    \n",
    "##  # no other time is allowed\n",
    "    else: \n",
    "        raise SystemExit\n",
    "\n",
    "\n",
    "##  # ==========================\n",
    "##  # webscrapping\n",
    "##  # ==========================\n",
    "##  # defines profile and browser\n",
    "    if driver=='Firefox':\n",
    "##      # profile for autosaving (Firefox)\n",
    "        pf = wd.FirefoxProfile()\n",
    "        pf.set_preference(\"browser.download.folderList\", 2)\n",
    "        pf.set_preference(\"browser.download.manager.showWhenStarting\", False)\n",
    "        pf.set_preference(\"browser.download.dir\", save_dir)\n",
    "        pf.set_preference(\"browser.helperApps.neverAsk.saveToDisk\", ft)\n",
    "        pf.set_preference(\"browser.link.open_newwindow\", 1)\n",
    "        \n",
    "##      # start webdriver\n",
    "        browser = wd.Firefox(pf)\n",
    "\n",
    "    elif driver=='Chrome':\n",
    "##      # WORK IN PROGRESS\n",
    "##      # profile for autosaving (Chrome)\n",
    "        pf = wd.ChromeOptions()\n",
    "        prefs = {'download.prompt_for_download': False,\n",
    "                 'safebrowsing.enabled': False,\n",
    "                 'safebrowsing.disable_download_protection': True,\n",
    "                 \"profile.default_content_settings.popups\": 0,\n",
    "                 \"download.default_directory\": save_dir,\n",
    "                 'download.directory_upgrade': True,}\n",
    "        pf.add_experimental_option('prefs', prefs)\n",
    "\n",
    "##      # start webdriver \n",
    "        browser = wd.Chrome(pf)\n",
    "        \n",
    "    else:\n",
    "        raise SystemExit\n",
    "\n",
    "##  # enter main page\n",
    "    browser.get(wm)\n",
    "    time.sleep(10)\n",
    "    \n",
    "    xp = \"/html/body/navbar/header[1]/nav/div/ul/li[2]/a\"\n",
    "    handle_noclick_xp(b=browser, xp_=xp)\n",
    "    time.sleep(10)\n",
    "    \n",
    "    username = browser.find_element_by_id(\"AAA-AS-SI1-SE003\")\n",
    "    password = browser.find_element_by_id(\"AAA-AS-SI1-SE006\")\n",
    "    username.send_keys(un)\n",
    "    password.send_keys(pw)\n",
    "    time.sleep(1)\n",
    "    browser.find_element_by_id(\"AAA-AS-SI1-SE014\").click()\n",
    "    time.sleep(10)\n",
    "\n",
    "##  # accept cookies\n",
    "    xp = \"/html/body/div[2]/div/div/div[2]/a\"\n",
    "    browser.find_element_by_xpath(xp).click()\n",
    "    time.sleep(3)\n",
    "\n",
    "##  # run through all info available\n",
    "    for i in obs: # df.shape[0]\n",
    "        \n",
    "##      # to check evolution\n",
    "        print('start ' + str(i) + ' of ' + str(df.shape[0]) + ':',\n",
    "              df.OrgName_1[i] )\n",
    "        \n",
    "##      # skip no info\n",
    "        if not( nan_org[i] ):\n",
    "            \n",
    "##          # get the web (depending on the quality of match)\n",
    "            if not(nan_org1[i]) and nan_org2[i]:\n",
    "                web = df.OpenPermID_1[i]\n",
    "            elif nan_org1[i] and not(nan_org2[i]):\n",
    "                web = df.OpenPermID_2[i]\n",
    "            elif df.Score_1[i] >= df.Score_2[i]:\n",
    "                web = df.OpenPermID_1[i]\n",
    "            else:\n",
    "                web = df.OpenPermID_2[i]\n",
    "                \n",
    "##          # enter web\n",
    "            browser.get(web)\n",
    "            time.sleep(15)\n",
    "\n",
    "##          # check for error\n",
    "            handle_web(b=browser, w=web)\n",
    "\n",
    "##          # get main info           \n",
    "            web_source = browser.page_source\n",
    "            soup = bs(web_source, 'html.parser')\n",
    "            full_class = soup.find_all('a', class_='link ng-binding')\n",
    "\n",
    "##          # if there is info go in                                    \n",
    "            if len(full_class) > 0:\n",
    "                full_info = []\n",
    "                for each_class in full_class:\n",
    "                    full_info.append(each_class.text)\n",
    "                if len(full_info) > 5: # when public, there is more info\n",
    "                    del full_info[5:]\n",
    "                if len(full_info) == 4:\n",
    "                    full_info.append('')\n",
    "\n",
    "##              # click section of interest\n",
    "                xp_s = \"/html/body/div[3]/section/div/div[2]/div[1]/div[\"\n",
    "                xp_e = \"]/div[2]/a\"\n",
    "                handle_noclick_extra(b=browser,\n",
    "                                     xp_start=xp_s,\n",
    "                                     xp_end=xp_e,\n",
    "                                     divs=range(5,8))\n",
    "                time.sleep(15)\n",
    "\n",
    "##              # get primary industry description\n",
    "                web_source = browser.page_source\n",
    "##                handle_web(b=browser, w=web) # not implemented yet\n",
    "                soup = bs(web_source, 'html.parser')\n",
    "                full_class = soup.find_all('div', class_='col-md-8 ng-binding')\n",
    "##                print(full_class)\n",
    "\n",
    "##              # if there is info go in                                  \n",
    "                if len(full_class) > 0:\n",
    "                    \n",
    "##                  # save info\n",
    "                    full_info.append(full_class[2].text) # TRBC code\n",
    "                    full_info.append(full_class[3].text) # Industry Description\n",
    "                    df.iloc[i, range(11,18)] = full_info\n",
    "            \n",
    "##                  # save data\n",
    "                    df.to_csv(full_path, index=False, encoding=encoding['encoding'])\n",
    "##                    print(full_info)\n",
    "                    \n",
    "##  # close user session\n",
    "    handle_noclick_id(b=browser, id_='profile-toggle')\n",
    "    handle_noclick_xp(b=browser, xp_='/html/body/navbar/header[1]/nav/div/div[2]/ul/li[3]/a')\n",
    "    time.sleep(3)\n",
    "    print('session closed')\n",
    "            \n",
    "##  # finish the page\n",
    "    browser.quit()\n",
    "    print('finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.5 manual identification of missing companies**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Manual identification of TRBC code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  Based on EU taxonomy, TRBC code is given to sift green industries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Merge data of companies and EU taxonomy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define functions `merge_taxonomy` to perform concatenation based on first 6 digits of TRBC code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "# function merge_taxonomy\n",
    "############################################\n",
    "def merge_taxonomy(file_match, file_taxonomy, file_main, save_dir):\n",
    "\n",
    "##  #-----------------------\n",
    "##  # taxonomy data files\n",
    "##  #-----------------------\n",
    "##  # path for file\n",
    "    full_path = file_taxonomy + 'sustainable-taxonomy_renewable.csv'\n",
    "        \n",
    "##  # load data\n",
    "    df_green = pd.read_csv(full_path, sep=',')\n",
    "    df_green['TRBC_6'] = df_green['TRBC_6'].astype(float).astype(str)\n",
    "    # print( df_green )\n",
    "    \n",
    "##  #-----------------------\n",
    "##  # match data files\n",
    "##  #-----------------------\n",
    "##  # path for file\n",
    "    full_path1 = file_match + '5_info_match_round2_complete.csv'\n",
    "    full_path2 = file_match + '6_info_match_round2_missing.csv'\n",
    "        \n",
    "##  # load data\n",
    "    df_comp = pd.read_csv(full_path1, sep=',')\n",
    "    df_miss = pd.read_csv(full_path2, sep=',')\n",
    "##    print(df_comp)\n",
    "##    print(df_miss)\n",
    "\n",
    "##  # append data\n",
    "    df_comp = df_comp.append(df_miss)\n",
    "    df_comp['TRBC code'] = df_comp['TRBC code'].astype(float).astype(str)\n",
    "##    print(df_comp)\n",
    "\n",
    "##  # create 'green' economy column\n",
    "    df_comp = df_comp.assign( green=df_comp['TRBC code'].isin(df_green.TRBC_6).astype(int) )\n",
    "##    print( sum(df_comp.green) )\n",
    "\n",
    "##  # save dataFrame\n",
    "    full_path = save_dir + '7_info_match_round3_complete.csv'\n",
    "    df_comp.to_csv(full_path, index=False)\n",
    "    \n",
    "\n",
    "##  #-----------------------\n",
    "##  # main data files\n",
    "##  #-----------------------\n",
    "##  # path for file\n",
    "    full_path = file_main + '1_CSPPholdings_201706_2021.csv'\n",
    "        \n",
    "##  # load data\n",
    "    df_main = pd.read_csv(full_path, sep=',')\n",
    "\n",
    "##  # merge 'TRBC code' and 'green' columns\n",
    "    right_df = df_comp[['Name_1','Name_2','TRBC code','green']]\n",
    "    right_df = right_df.rename(columns={\"Name_1\":\"Name1\", \"Name_2\":\"Name2\"})\n",
    "    df_main = df_main.merge(right_df, on=['Name1','Name2'], how='left')\n",
    "##    print(df_main)\n",
    "\n",
    "##  # save dataFrame\n",
    "    full_path = save_dir + '7_CSPPholdings_201706_2021.csv'\n",
    "    df_main.to_csv(full_path, index=False)\n",
    "\n",
    "    print('finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA COLLECTION DONE!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
